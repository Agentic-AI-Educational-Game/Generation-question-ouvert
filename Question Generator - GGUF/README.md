# ğŸ§  Question Generation API with Hugging Face Models

Une application **Flask** qui utilise des modÃ¨les GGUF de Hugging Face pour gÃ©nÃ©rer des questions pertinentes Ã  partir d'un texte fourni.

---

## ğŸ–¼ï¸ Stack Technologique

| Technologie | Description |
|-------------|-------------|
| **Python** | Langage principal de l'application |
| **Flask** | Framework web pour crÃ©er des API REST |
| **llama_cpp** | BibliothÃ¨que pour exÃ©cuter des modÃ¨les GGUF localement |
| **Hugging Face Hub** | TÃ©lÃ©chargement de modÃ¨les depuis le Hub HF |
| **SentencePiece** | Tokenizer pour modÃ¨les LLaMA (inclus dans llama_cpp) |

---

## ğŸš€ Fonctionnement

### Ã‰tapes du processus :

1. **ğŸ“¥ TÃ©lÃ©chargement du modÃ¨le** :  
   Au premier dÃ©marrage, l'application tÃ©lÃ©charge le modÃ¨le GGUF depuis Hugging Face Hub.

2. **âš™ï¸ Initialisation** :  
   Le modÃ¨le est chargÃ© en mÃ©moire avec une configuration optimisÃ©e pour la gÃ©nÃ©ration de questions.

3. **ğŸ“ GÃ©nÃ©ration de questions** :  
   Quand l'API reÃ§oit un texte :
   - PrÃ©pare un prompt structurÃ© en franÃ§ais
   - ExÃ©cute le modÃ¨le avec un stream de tokens
   - Extrait la question du format de rÃ©ponse spÃ©cifiÃ©

4. **ğŸ“¤ Retour du rÃ©sultat** :  
   Renvoie la question gÃ©nÃ©rÃ©e au format JSON avec :
   - La question extraite
   - La rÃ©ponse brute du modÃ¨le

---

## ğŸ“® Point de Terminaison API

### `POST /generate_question`

**RequÃªte JSON :**
```json
{
  "text": "Le cycle de l'eau comprend trois Ã©tapes principales : Ã©vaporation, condensation et prÃ©cipitation. L'Ã©vaporation se produit lorsque l'eau des ocÃ©ans se transforme en vapeur..."
}
```

**RÃ©ponse JSON :**
```json
{
  "question": "Quelles sont les trois Ã©tapes principales du cycle de l'eau ?",
  "raw": "**Question :** Quelles sont les trois Ã©tapes principales du cycle de l'eau ?"
}
```

---

## âš™ï¸ Configuration & ExÃ©cution

1. Installez les dÃ©pendances :
```bash
pip install flask huggingface-hub llama-cpp-python
```

2. ExÃ©cutez l'application :
```bash
python app.py
```

3. L'application sera accessible sur :
```
http://localhost:5000
```

---

## ğŸ“ Structure des Fichiers

```
.
â”œâ”€â”€ app.py                # Application principale
â”œâ”€â”€ models/               # Dossier des modÃ¨les tÃ©lÃ©chargÃ©s
â”‚   â””â”€â”€ finetuned-qwen2.5-0.5B_instruct_finetuned_fr.q8_0.gguf
```

---

## âš ï¸ Notes Importantes

- **TÃ©lÃ©chargement initial** : Le premier dÃ©marrage peut prendre plusieurs minutes pour tÃ©lÃ©charger le modÃ¨le (~2-4GB)
- **MÃ©moire requise** : L'application nÃ©cessite au moins 4GB de RAM
- **ModÃ¨le utilisÃ©** : `zinec/finetuned-qwen-fr` (Qwen 0.5B quantifiÃ© en 8-bit)
- **Langue** : SpÃ©cialement conÃ§u pour gÃ©nÃ©rer des questions en franÃ§ais

---

## ğŸ’¡ Cas d'Usage

**Ã‰ducation** :  
- GÃ©nÃ©ration automatique de questions d'Ã©valuation  
- CrÃ©ation de questionnaires Ã  partir de contenus pÃ©dagogiques  
- Outil d'aide Ã  la prÃ©paration d'examens  

**Contenu** :  
- GÃ©nÃ©ration de FAQ Ã  partir d'articles  
- CrÃ©ation de quiz interactifs  
- Augmentation de datasets pour NLP
